\chapter{Transfer Function Optimization Using Visibility-Weighted Saliency}

\section{Introduction}
Volume visualization is an effective means of discovering meaningful features in volume data sets.
Both the exterior and interior of structures can be revealed simultaneously in a semi-transparent manner by specifying opacity values for the features in transfer functions.
Features can be intensity intervals in 1D transfer functions, rectangular or other shapes in 2D or higher-dimensional transfer functions.

In the specification of transfer functions for volume visualization, users often have a rough idea of how clear and opaque each feature should be and then adjust the opacity value of the features accordingly.
However, the relationship between the opacity of features and the saliency of the features in the final image is not linear.
The saliency of a feature in the final image depends on the opacity value assigned to the feature as well as the neighborhood of the feature and view-dependent occlusion of the feature.

Therefore, it is desirable to have an automated method to assist the user in the design of transfer functions. In this chapter, we propose an optimization approach to automatically refine a user-defined transfer function towards target saliency levels specified by the user.

\section{Related Work}

%-------------------------------------------------------------------------
\section{Method}
In Chapter~\ref{visibility-weighted_saliency}, visibility-weighted saliency was proposed as a measure of visual saliency of features in volume rendered images, in order to assist users in choosing suitable viewpoints and designing effective transfer functions to visualize the features of interest. In this chapter, we describe a transfer function optimization approach based on the visibility-weighted saliency metric, which indicates the perceptual importance of voxels and the visibility of features in volume rendered images.

The approach described in Chapter~\ref{transfer_function_refinement} is an automated method of optimizing transfer functions, based on the intensity distribution of voxels in the volume data set. However, this approach does not take into account the spatial distribution of voxels and the viewpoint of the visualization. Visibility-weighted saliency, on the other hand, takes into account both of these two aspects. The visibility-weighted saliency consists of two component fields, i.e. saliency field and visiblity fields. Saliency fields are essentially difference of Gaussians, which include the information of local neighborhoods of voxels, and visibility fields are computed from opacity contribution of voxels to volume rendered images, which indicate viewpoint dependent occlusions of the voxels.

Constraints are introduced in the search of the parameter space. Only the opacity of features would be changed in the transfer function domain. The definition of features (e.g. the data intervals) and the colors of features remain the same.
These constraints are based on the assumption that the user has explored the volume data and set up the transfer function according to his/her needs. Our algorithm aims to help the user reduce occlusion while preserving the user's knowledge or judgments of the data set.

\subsection{Objective Function}
Users define target importance values for each feature defined in the transfer function domain.
Our approach adjusts the transfer function to match the visibility-weighted saliency with the user-defined target saliency values.
Multiple saliency fields computed from different appearance attributes can be combined together in order to represent different aspects of the visual saliency of voxels.
In our implementation, brightness and saturation are used respectively to compute visibility-weighted saliency fields and define the weighted sum of the two sets of feature saliency as visibility-weighted feature saliency.
The objective function is defined as the root mean square of the differences of the visibility-weighted saliency and target importance of each feature.
\[ f=\sqrt{ \frac{\sum_{i=1}^{n} (W_{i}-t_{i})^{2}}{n} } 
\addtag \]
where $ W_{F}=u_{1}W_{F}(O_{b},i,\sigma)+u_{2}W_{F}(O_{s},i,\sigma) $ is the visibility-weighted saliency of feature $ i $, and $ t_{i} $ is the user-defined importance of feature $ i $. These user-defined saliency values are normalized and they add up to 1, in other words, $ t_{i} \in [0.1] $ and $ \sum_{i=1}^{n} t_{i} = 1 $.

As previously described in Section~\ref{weighted_feature_saliency}, multiple saliency fields computed from different appearance attributes can be combined together in order to represent different aspects of the visual saliency of voxels.
In our implementation, $ W_{F}=u_{1}W_{F}(O_{b},i,\sigma)+u_{2}W_{F}(O_{s},i,\sigma) $ is a weighted sum of visibility-weighted saliency values computed using brightness and saturation of voxels respectively, and $ u_{1} $ and $ u_{2} $ are weights of the two appearance attributes.

However, the visibility-weighted saliency $ W_{i} $ is not a variable that can be directly modified. Instead, $ W_{i} $ is a complicated function of the color and opacity of voxels in feature $ i $ and is also influenced by the viewpoint of rendering. A visibility-weighted saliency field is a combination of a visibility field and a saliency field. The saliency field is a view-independent field based on the color of every voxels in the volume data set, while the visibility field is a view-dependent field computed from the opacity contribution of every voxels to the final image when rendered from a certain viewpoint.

The computation of visibility fields is non-trivial. In order to compute a visibility field, a slice-based rendering is performed on a series of quads which are parallel to the viewing plane, one for each slice.
Subsequently, the visibility values are computed by subtracting the accumulated opacity of the previous slice from that of the current slice. After collecting the visibility values of all voxels, the visibility field can be constructed. The details of visibility fields were previously described in Section~\ref{visibility_fields}.

The evaluation of the objective function is computational expensive. However, in an iterative optimization, the visibility field and visibility-weighted saliency have to be recompute at each step after the feature opacity value are updated.

\subsection{Parameter Space}
We use a nucleon data set to demonstrate how the visibility-weighted saliency of features change when the feature opacity values change. As displayed in Figure~\ref{fig:nucleon_naive}, three features are defined in the transfer function for the nucleon data set.

The dimension of the parameter space is the same as the number of features defined by the user. In this case, three features are defined for the nucleon data set. The opacity of each feature is mapped to an axis in the parameter space. Therefore, the opacity values of the 3 features are mapped to $ x, y, z $ axes of a 3D scalar field. Figure~\ref{fig:nucleon_densityplot} displays three 3D scalar fields, one for each feature, to provide an intuitive overview of the relationship between the feature opacity values and the visibility-weighted saliency values.

Feature 1 (the purple structure in Figure~\ref{fig:nucleon_naive}) is the exterior of the nucleon data set, the visibility-weighted saliency of this feature is shown in the 3D scalar fields in the same color at the left in Figure~\ref{fig:nucleon_densityplot}.
The visibility-weighted saliency of feature 1 increases as its opacity increases, as shown in the 3D scalar fields that the brightness and opacity increase along $ x $ axis. Similar patterns also appear in the other 2 scalar fields, the visibility-weighted saliency of feature 2 (the red structure in Figure~\ref{fig:nucleon_naive}) and feature 3 (the green structure in Figure~\ref{fig:nucleon_naive}) also increase as their opacity increase.
Thus it is reasonable to assume that the visibility-weighted saliency of a feature is a monotonic function of its opacity.

Moreover, feature 1 is the exterior of the nucleon, its visibility-weighted saliency is almost not influenced by the opacity of other features. On the other hand, the visibility-weighted saliency of feature 2 is influenced by both the opacity of feature 1 and feature 2. In addition, the visibility-weighted saliency of feature 3 is drastically influenced by the opacity of feature 1, feature 2 and feature 3, as feature 3 is an interior structure and can be easily occluded by the other 2 features.

In order to demonstrate the distribution of the objective function in the parameter space $ x \in [0,1] $, $ y \in [0,1] $ and $ z \in [0,1] $, we sample the parameter space with sampling interval $ 0.1 $, from 0 to 1 along each axis. There are 11 sampling points along each axis, which results in 1331 sampling points in the parameter space. In Figure~\ref{fig:nucleon_parameterspace}, the parameter space is rendered as a density plot with a temperate color map which gradually changes from orange to blue.

\begin{figure}
\centering
	\begin{minipage}{.3\textwidth}
	\includegraphics[width=1\linewidth]{images/nucleon_naive}
	\end{minipage}~
	\begin{minipage}{.3\textwidth}
	\includegraphics[width=1\linewidth]{images/tf_nucleon_naive}
	\end{minipage}
	\caption{A nucleon data set \cite{website:Voreen_datasets_2013} (left) with a transfer function of equal opacity values to the 3 features (right)}
	\label{fig:nucleon_naive}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/densityplot1}	
	\end{minipage}~
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/densityplot2}	
	\end{minipage}
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/densityplot3}	
	\end{minipage}
	\caption{Visibility-weighted saliency of the 3 features are mapped to brightness and opacity of the 3D fields at the left, middle and right respectively. The visibility-weighted saliency of feature 2 (red) is affected by both the opacity of feature 1 (purple) and feature 2. The visibility-weighted saliency of feature 3 (green) is affected by the opacity of feature 1, feature 2 and feature 3.}
	\label{fig:nucleon_densityplot}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{.6\textwidth}
		\includegraphics[width=1\linewidth]{images/parameterspace}
	\end{minipage}
	\caption{The values of the objective function $  f $ is displayed as the density in the parameter space (with sampling interval $ 0.1 $). Target saliency values are set to 0.1, 0.3 and 0.6 for the 3 features respectively. Only the high and low values are visible and the data range in the middle is set to transparent.}
	\label{fig:nucleon_parameterspace}
\end{figure}

\subsection{Optimization Algorithm}
The gradient descent algorithm is employed in our optimizer.
Gradient descent is a first-order optimization algorithm. It is based on the observation that if an objective function $ f(x) $ is defined and differentiable in a neighborhood of a point $ x_{1} $, then $ f(x) $ decreases fastest in the direction of the negative gradient of the objective function.

Given a continuously differentiable function $ f(x) $ with $ x \in \mathbb{R}^{n} $, let $ x_{k} $ be the current iteration point and $ g_{k}=g(x_{k})= \nabla f(x_{k}) $ be the gradient of $ f(x) $ at $ x_{k} $. The gradient descent method defines the next iteration point by
\[ x_{k+1}=x_{k}- \alpha_{k} g_{k} , k \geq 0 \]
for $ \alpha_{k} $ small enough, then $ f(x_{k+1}) \leq f(x_{k}) $. The gradient varies as the iteration proceeds, tending to zero as it approaches a local minimum. When the gradient decreases, the iteration step sizes also decrease. So hopefully the sequence $ {x_{k}} $ converges to the desired local minimum after performing the iteration.

In gradient descent methods, we can either take very small step sizes and reevaluate the gradient at every steps, or take large steps each time. If the step size is too small, it may end up in a laborious situation that the objective function converges very slowly. If the step size is too large, it results in a more zigzag path and may have the risk of missing the local minimum and thus cannot converge \cite{chong_introduction_2013}.

\subsection{Gradient Estimation}
In terms of gradient estimations, we have experimented with two methods. First, we have tried to estimate the gradient with a backward difference divided by a small step.
\[ \frac{\nabla_{h}[f](x)}{h}=\frac{f(x)-f(x-h)}{h} \]
where $ h $ is nonzero number.

When $ h $ is small, the backward difference divided by $ h $ approximates the derivative. The error in this approximation can be derived from Taylor's theorem. Assuming that $ f $ is differentiable, we have
\[ \frac{\nabla_{h}[f](x)}{h}-f'(x)=\mathcal{O}(h) \to 0 , \; as \; h \to 0 \]
Because the evaluation of the objective function is computational expensive, in our implementation, we use the $ h $ as the step size and $ f(x-h) $ would be the objective function value from the last iteration. Thus no extra evaluation of the objective function is required.

\subsection{Adaptive Step Size with Line Search}
Various approaches have been proposed regarding the choices of step sizes, which lead to various gradient algorithms \cite{yuan_step-sizes_2008}.

The line search strategy adapts the step size in gradient descent in order to achieve a reduction in the objective function while still making sufficiently fast progress. Line search in employed in many multivariate optimization algorithms.

\[ h(\alpha_{k})=f(x_{k}+\alpha_{k} g_{k}) \]

The exact line search algorithm chooses the next iteration point by achieving the least objective function value. However, despite the optimal properties, exact line search often behave poorly and tend to zigzag in two orthogonal directions, which usually implies deteriorations in converge \cite{zhou_gradient_2006}.

\cite{vrahatis_class_2000}
\cite{armijo_minimization_1966}

The input to a line search is an objective function $ f $, an initial point $ x $, direction $ d $ and an initial step size $ s $.

Choose a small step size and step iteratively step in the opposite direction of the gradient, evaluate the objective function at each step, and stop when the object function increases, then back up one step.
This does not find the exact minimum along the line direction, in stead it yields reasonable results and runs very quickly.

\[ f(x+ \gamma d)<f(x) \]
\[ \gamma_{n}=\gamma_{1}2^{n} \]

Figure~\ref{fig:nucleon_parameterspace_path}

\subsection{Parallel Line Search}
Parallel computation has overhead.
It's worthy to perform line search in parallel when the evaluation of the objective function is computational expensive.
Computing the visibility fields is require to perform a pass of slice-based volume rendering, which is computational expensive.

%-------------------------------------------------------------------------
\section{Results and Discussions}

\begin{figure}
	\centering
	\begin{minipage}{.9\textwidth}
		\includegraphics[width=1\linewidth]{images/parameterspace_path}
	\end{minipage}
	\caption{The footprints of gradient descent methods with fixed step size and adaptive step size are shown in the parameter space in Figure~\ref{fig:nucleon_parameterspace}}
	\label{fig:nucleon_parameterspace_path}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/saliency_fixed}
		\caption{a}	
	\end{minipage}~
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/saliency_linesearch}
		\caption{b}	
	\end{minipage}
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/saliency_parallelsearch}
		\caption{b}	
	\end{minipage}
	\label{fig:nucleon_saliency}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/opacity_fixed}
		\caption{a}	
	\end{minipage}~
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/opacity_linesearch}
		\caption{b}	
	\end{minipage}
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/opacity_parallelsearch}
		\caption{b}	
	\end{minipage}
	\label{fig:nucleon_opacity}
\end{figure}

\begin{figure}
	\centering
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/rms_fixed}
		\caption{a}	
	\end{minipage}~
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/rms_linesearch}
		\caption{b}	
	\end{minipage}
	\begin{minipage}{.3\textwidth}
		\includegraphics[width=1\linewidth]{images/rms_parallelsearch}
		\caption{b}	
	\end{minipage}
	\label{fig:nucleon_rms}
\end{figure}

%-------------------------------------------------------------------------
\section{Conclusions}
